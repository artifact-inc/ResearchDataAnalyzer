{"id": "rdla_20251118_000603", "tier": "B", "data_type_name": "Frequency Estimation Dataset", "value_score": 7.299999999999999, "confidence_score": 9.0, "dataset_description": "The research does not appear to use a specific dataset, but rather focuses on developing a new frequency estimation algorithm using a Residue Number System (RNS) and differential privacy techniques.", "data_collection_method": "Not applicable, as the research does not use a specific dataset.", "replication_feasibility": "High, as the proposed algorithm can be implemented and tested using publicly available tools and libraries. However, the feasibility of applying the technique to real-world datasets may depend on the specific data sources and regulatory requirements.", "paper_url": "https://arxiv.org/abs/2511.11569v1", "paper_title": "Private Frequency Estimation Via Residue Number Systems", "detected_at": "2025-11-18T00:06:03.154088+00:00"}
{"id": "rdla_20251118_000618", "tier": "B", "data_type_name": "Object hallucination detection", "value_score": 6.8, "confidence_score": 7.0, "dataset_description": "The paper uses synthetic datasets to evaluate the PAS method, including the Hallucinated Objects in VQA (HOV) and the Visual Madlibs (VML) datasets.", "data_collection_method": "The synthetic datasets were generated by introducing object hallucinations into existing vision-language datasets.", "replication_feasibility": "medium. While the synthetic datasets used in the paper are accessible, replicating the object hallucination patterns in real-world datasets would require significant effort and access to proprietary model training data.", "paper_url": "https://arxiv.org/abs/2511.11502v1", "paper_title": "PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models", "detected_at": "2025-11-18T00:06:18.181943+00:00"}
{"id": "rdla_20251118_000654", "tier": "B", "data_type_name": "Digitized histopathology slides", "value_score": 6.8, "confidence_score": 7.0, "dataset_description": "The paper mentions the use of a 90-question benchmark dataset called SlideQuest, which was verified by pathologists and biomedical scientists. However, the specific details of the histopathology slides used in this dataset are not provided.", "data_collection_method": "The paper does not provide details on how the SlideQuest dataset was obtained or curated. It is unclear if the slides were manually annotated by experts, collected from existing sources, or generated synthetically.", "replication_feasibility": "medium - Obtaining a comprehensive dataset of digitized histopathology slides may require partnerships with medical institutions and careful consideration of privacy and regulatory concerns. The process of curating a benchmark dataset like SlideQuest would also require significant effort and domain expertise.", "paper_url": "https://arxiv.org/abs/2511.11324v1", "paper_title": "NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery", "detected_at": "2025-11-18T00:06:54.541364+00:00"}
{"id": "rdla_20251118_000658", "tier": "B", "data_type_name": "Simulated social navigation data", "value_score": 6.449999999999999, "confidence_score": 9.0, "dataset_description": "The paper used simulated data from a rule-based social locomotion model to train the reinforcement learning framework. The specific details of the dataset are not provided.", "data_collection_method": "The data was generated through simulations based on a rule-based social locomotion model, which was grounded in empirical behavioral experiments.", "replication_feasibility": "medium. Replicating the dataset would require access to the specific simulation environment and implementation of the rule-based social locomotion model, which may not be easily accessible. Additionally, the paper does not provide detailed information on the dataset, making it challenging to reproduce the experiments.", "paper_url": "https://arxiv.org/abs/2511.11323v1", "paper_title": "RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms", "detected_at": "2025-11-18T00:06:58.222367+00:00"}
{"id": "rdla_20251118_000701", "tier": "A", "data_type_name": "Financial language models", "value_score": 7.85, "confidence_score": 7.0, "dataset_description": "The paper does not provide specific details on the datasets used for training and evaluation. It mentions the use of the FLARE and FLARE-ES benchmarks, which are financial NLP datasets, as well as unnamed 'financial NLP tasks'.", "data_collection_method": "The data collection method is not explicitly described in the paper. It's likely that the FLARE and FLARE-ES benchmarks were obtained through web scraping or partnerships with financial institutions, but the details are unclear.", "replication_feasibility": "Medium. The use of financial NLP benchmarks suggests the data may not be publicly available, requiring partnerships or moderate effort to obtain. The lack of specific details on the datasets used makes it difficult to assess the feasibility of replication.", "paper_url": "https://arxiv.org/abs/2511.11315v1", "paper_title": "LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models", "detected_at": "2025-11-18T00:07:01.525623+00:00"}
{"id": "rdla_20251118_000716", "tier": "B", "data_type_name": "Visual Concept Unlearning Benchmark (VCUBench)", "value_score": 7.299999999999999, "confidence_score": 9.0, "dataset_description": "VCUBench, a benchmark dataset constructed by the authors to evaluate visual concept unlearning, containing both synthetic and real-world visual data", "data_collection_method": "A combination of synthetic data generation and web scraping of real-world visual content", "replication_feasibility": "Medium. Constructing a comprehensive visual benchmark dataset requires significant effort, though the authors' approach provides a starting point. Obtaining permissions for web-scraped real-world data may also present challenges.", "paper_url": "https://arxiv.org/abs/2511.11299v1", "paper_title": "AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models", "detected_at": "2025-11-18T00:07:16.032746+00:00"}
